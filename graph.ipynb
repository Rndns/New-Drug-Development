{"cells":[{"cell_type":"markdown","metadata":{},"source":["HLM 및 MLM은 간 및 마우스의 간 대사효소와 화합물을 30분 동안 반응시킨 후, \n","<대사되지 않고 남아있는 화합물의 양>을(%) LC-MS/MS로 측정함으로써 화합물의 간 대사효소에 대한 안정성을 평가한 데이터\n","LC-MS/MS : 액체 크로마토그래피 질량 분석(액체 크로마토그래피의 물리적 분리 기능과 질량 분석 기능을 결합한 분석 화학 기술)\n","복잡한 샘플에서 특정 화합물을 분리, 정량 및 식별하는데 널리 사용\n","\n","대사 안정성이 낮은 물질의 경우 체내 소실이 빨리, 많이 된다.\n","적절히 체내에 남아 있으면서 약효가 발휘되어야한다.\n","가능한 한 신약개발 초기부터 대사안정성이 높은 물질을 선별해야한다.\n","\n","신약개발의 metabolity 최적화 -> 실험적인 방법과 컴퓨터를 활용한 전략을 병용한 사례가 있음.\n","일부 약물의 경우 clearance가 높았다고...\n","\n","------------------------------------------------------------------------------------------------\n","\n","간 조직을 분리해서 얻은 hepatocyte가 metabolism을 평가하는데 가장 좋지만,\n","S9, Microsome, cytosol도 있음\n","간 조직을 경질화 한다음, 낮은 speed로 원심분리하여 얻은 것이 s9,\n","그 다음 빠른 속도로 원심분리하여, 위에 뜬게 cytosol, 아래 가라앉은게 microsomal\n","가장 대표적인 enzyme을 포함한 microsome을 이용한 대사 평가가 가장 많이 사용\n","liver Microsomes in Buffer(Pb 7.4) + Drug -> (NDAPH, 37도에서, CYP450) -> Drug + Drug -OH\n","\n","주최측에서 준 것은 Single point assay(with / without NADPH가 가능)\n","\n","대사 안정성 분류 기준\n","반감기 기준으로 할 경우, 30분 기준 <50% 이면 안정한 물질, 중간은 moderate, <12.4%면 불안정한 물질.\n","clearance 기준으로 할 경우, ~\n","다만 우리가 가진건 단지 30분 후 대사물의 양일뿐\n","\n","------------------------------------------------------------------------------------------------\n","\n","column들의 의미\n","\n","id: 화합물의 아이디.\n","SMILES: 화합물의 SMILES (Simplified Molecular Input Line Entry System) 표기법.\n","MLM: 화합물의 MLM (Microsomal Liver Metabolism) 값.\n","HLM: 화합물의 HLM (Human Liver Microsomes) 값.\n","AlogP: 화합물의 AlogP 값.\n","Molecular_Weight: 화합물의 분자량.\n","Num_H_Acceptors: 화합물의 수소 수용체 개수.\n","Num_H_Donors: 화합물의 수소공여체 개수.\n","Num_RotatableBonds: 화합물의 회전 가능 결합 개수.\n","LogD: 화합물의 LogD 값.\n","Molecular_PolarSurfaceArea: 화합물의 극성 표면적.\n","\n","------------------------------------------------------------------------------------------------\n","\n","Lipinski's Rule\n","\n","- **Rule of five(RO5)** 라고도 불리며 Druglikeness를 판단하는데 사용\n","- 1997년 Pfizer의 과학자 Christopher A. Lipinski는 구강복용하는 약물이 상대적으로 크기가 작고 지용성이라는 것을 관찰하여 이 규칙을 고안\n","\n","- ADME(pharmacokinetic)에 기반한 프로파일\n","- Molecular weight < 500 Dalton\n","- Octanol-water partition coefficient (LogP) < 5\n","- Hydrogen bond donors < 5\n","- Hydrogen bond acceptors < 10\n","\n","------------------------------------------------------------------------------------------------"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n","Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"]}],"source":["import sys\n","import pandas as pd\n","import numpy as np\n","import os\n","import operator\n","import string\n","import re\n","import random\n","import warnings\n","warnings.filterwarnings('ignore')\n","import platform\n","import json, pickle\n","import networkx as nx\n","\n","from copy import deepcopy\n","from math import sqrt\n","from random import shuffle\n","from collections import OrderedDict\n","from scipy import stats\n","from IPython.display import SVG\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Sequential, Linear, ReLU\n","\n","from rdkit import Chem\n","from rdkit.Chem.Draw import IPythonConsole, rdMolDraw2D\n","from rdkit.Chem import AllChem, rdDepictor, MolFromSmiles, Descriptors, rdMolDescriptors, Crippen, QED, EState, Lipinski, MolSurf, Fragments\n","\n","import deepchem as dc\n","from deepchem.feat.molecule_featurizers import MolGraphConvFeaturizer\n","from deepchem.feat import RDKitDescriptors\n","\n","import torch_geometric\n","from torch.nn import Dropout, BatchNorm1d\n","from torch_geometric.data import Dataset, Data\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.nn import GCNConv, GATConv, GINConv, global_add_pool, global_max_pool\n","\n","import sklearn as sk\n","from sklearn.utils import check_random_state\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import mean_squared_error\n","\n","import deepchem as dc\n","from deepchem.splits.splitters import ScaffoldSplitter\n","\n","# 시각화 라이브러리\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Python Platform: macOS-13.5.2-arm64-arm-64bit\n","PyTorch Version: 2.0.1\n","\n","Python 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ]\n","Pandas 2.1.0\n","Scikit-Learn 1.3.0\n","GPU is NOT AVAILABLE\n","Target device is cpu\n"]}],"source":["# What version of Python do you have?\n","has_gpu = torch.cuda.is_available()\n","# has_mps = torch.backends.mps.is_built()\n","device = \"cuda\" if has_gpu else \"cpu\"\n","\n","print(f\"Python Platform: {platform.platform()}\")\n","print(f\"PyTorch Version: {torch.__version__}\")\n","print()\n","print(f\"Python {sys.version}\")\n","print(f\"Pandas {pd.__version__}\")\n","print(f\"Scikit-Learn {sk.__version__}\")\n","print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n","print(f\"Target device is {device}\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["CFG = {\n","    'SEED': 42,\n","    'TRAIN_BATCH_SIZE' : 32,\n","    'TEST_BATCH_SIZE' : 32,\n","    'LR' : 0.0002,\n","    'LOG_INTERVAL' : 20,\n","    'NUM_EPOCHS' : 50,\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    check_random_state(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(CFG['SEED']) # Seed 고정"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>SMILES</th>\n","      <th>MLM</th>\n","      <th>HLM</th>\n","      <th>AlogP</th>\n","      <th>Molecular_Weight</th>\n","      <th>Num_H_Acceptors</th>\n","      <th>Num_H_Donors</th>\n","      <th>Num_RotatableBonds</th>\n","      <th>LogD</th>\n","      <th>Molecular_PolarSurfaceArea</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAIN_0000</td>\n","      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n","      <td>26.010</td>\n","      <td>50.680</td>\n","      <td>3.259</td>\n","      <td>400.495</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>3.259</td>\n","      <td>117.37</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TRAIN_0001</td>\n","      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n","      <td>29.270</td>\n","      <td>50.590</td>\n","      <td>2.169</td>\n","      <td>301.407</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2.172</td>\n","      <td>73.47</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAIN_0002</td>\n","      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n","      <td>5.586</td>\n","      <td>80.892</td>\n","      <td>1.593</td>\n","      <td>297.358</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1.585</td>\n","      <td>62.45</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TRAIN_0003</td>\n","      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n","      <td>5.710</td>\n","      <td>2.000</td>\n","      <td>4.771</td>\n","      <td>494.652</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>3.475</td>\n","      <td>92.60</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRAIN_0004</td>\n","      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n","      <td>93.270</td>\n","      <td>99.990</td>\n","      <td>2.335</td>\n","      <td>268.310</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.337</td>\n","      <td>42.43</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id                                             SMILES     MLM  \\\n","0  TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n","1  TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n","2  TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n","3  TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n","4  TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n","\n","      HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n","0  50.680  3.259           400.495                5             2   \n","1  50.590  2.169           301.407                2             1   \n","2  80.892  1.593           297.358                5             0   \n","3   2.000  4.771           494.652                6             0   \n","4  99.990  2.335           268.310                3             0   \n","\n","   Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  \n","0                   8  3.259                      117.37  \n","1                   2  2.172                       73.47  \n","2                   3  1.585                       62.45  \n","3                   5  3.475                       92.60  \n","4                   1  2.337                       42.43  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv('Data/train.csv')\n","test = pd.read_csv('Data/test.csv')\n","train.head()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def one_of_k_encoding(x, allowable_set):\n","    if x not in allowable_set:\n","        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n","    return list(map(lambda s: x == s, allowable_set))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def atom_features(atom, mol): # 52\n","    features = one_of_k_encoding(atom.GetSymbol(),['N', 'S', 'F', 'Cl', 'Se', 'Br', 'O', 'C', 'I', 'P'])\n","    features += one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","    features += one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","    features += one_of_k_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","    features.append(atom.GetIsAromatic())\n","    features += one_of_k_encoding(atom.GetFormalCharge(), [-1, 0, 1])  # 형식 전하\n","    features += one_of_k_encoding(str(atom.GetHybridization()), ['SP', 'SP2', 'SP3', 'SP3D', 'SP3D2'])  # 혼성\n","    \n","    return np.array(features)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Returns : 원자 개수, 원자 특성 행렬, 인접 행렬\n","def smile_to_graph(smile):\n","    # SMILES 문자열로부터 분자 그래프 데이터 생성\n","    mol = Chem.MolFromSmiles(smile)\n","    \n","    # mol.GetNumAtoms() : 분자에 소속되어 있는 원자의 개수\n","    c_size = mol.GetNumAtoms()\n","    \n","    features = []\n","\n","    # 분자에 소속되어 있는 원자들을 순회하면서 원자 특성 정보 수집\n","    for atom in mol.GetAtoms():\n","        feature = atom_features(atom, mol)\n","        # 정규화?\n","        features.append( feature / sum(feature) )\n","\n","    edges = []\n","    # 분자를 이루는 원자들의 연결 구조 정보를 순회하면서 인접 정보 수집\n","    # 연결 구조 정보 : 시작 원자 index, 끝 원자 index\n","    for bond in mol.GetBonds():\n","        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n","\n","    # 연결 구조 정보를 통한 방향 그래프 생성(양방향)\n","    g = nx.Graph(edges).to_directed()\n","    edge_index = []\n","    for e1, e2 in g.edges:\n","        edge_index.append([e1, e2])\n","\n","    return c_size, features, edge_index"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["smile_graph = {}\n","for df in [train, test]:\n","    smiles = df['SMILES']\n","    for smile in smiles:\n","        g = smile_to_graph(smile)\n","        smile_graph[smile] = g"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["dataset = dc.data.NumpyDataset(X=np.array(train['SMILES']), ids=train['SMILES'].values)\n","splitter = dc.splits.ScaffoldSplitter()\n","train_dataset, valid_dataset = splitter.train_test_split(dataset, frac_train=0.85, seed=CFG['SEED'])\n","train_df = train[train['SMILES'].isin(train_dataset.ids)]\n","valid_df = train[train['SMILES'].isin(valid_dataset.ids)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TestbedDataset(Dataset):\n","    def __init__(self, df, mode='train', transform=None, pre_transform=None, smile_graph=None, scaler=None):\n","        super(TestbedDataset, self).__init__(root=None, transform=transform, pre_transform=pre_transform)\n","        self.mode = mode\n","        self.smile_graph = smile_graph\n","        self.scaler = scaler\n","        \n","        self.xd = df['SMILES'].tolist()\n","\n","        if self.mode == 'train':\n","            self.y = df[['MLM', 'HLM']].values.tolist()\n","        else:  # 'test' mode\n","            self.y = [None] * len(self.xd)\n","\n","        if self.mode == 'train':\n","            exclude_cols = ['SMILES', 'MLM', 'HLM']\n","            additional_data = df.drop(exclude_cols, axis=1)\n","        \n","        else:\n","            additional_data = df.drop('SMILES', axis=1)\n","\n","        if self.mode == 'train':\n","            self.scaler = StandardScaler()\n","            self.scaled_data = self.scaler.fit_transform(additional_data)\n","        elif self.mode == 'test' and self.scaler is not None:\n","            self.scaled_data = self.scaler.transform(additional_data)\n","        else:\n","            raise ValueError(\"Scaler should be provided for test mode.\")\n","\n","    def len(self):\n","        return len(self.xd)\n","\n","    def get(self, idx):\n","        smiles = self.xd[idx]\n","        labels = self.y[idx]\n","        additional_features = self.scaled_data[idx]\n","\n","        c_size, features, edge_index = self.smile_graph[smiles]\n","\n","        data = Data(x=torch.Tensor(features),\n","                    edge_index=torch.LongTensor(edge_index).transpose(1, 0))\n","        \n","        if labels:\n","            data.y = torch.FloatTensor([labels])\n","        data.c_size = torch.LongTensor([c_size])\n","        data.additional_features = torch.Tensor(additional_features)\n","\n","        return data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset = TestbedDataset(df=train_df, mode='train', smile_graph=smile_graph)\n","val_dataset = TestbedDataset(df=valid_df, mode='train', smile_graph=smile_graph, scaler=train_dataset.scaler)\n","test_dataset = TestbedDataset(df=test, mode='test', smile_graph=smile_graph, scaler=train_dataset.scaler)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch = next(iter(train_dataset))\n","batch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GATNet(torch.nn.Module):\n","    def __init__(self, n_output=2, num_features_xd=52, output_dim=128, dropout=0.2, num_additional_features=66):\n","        super(GATNet, self).__init__()\n","\n","        dim = 32\n","        heads = 4  # Number of attention heads\n","\n","        # 약물 분자 표현을 위한 GAT 층 구성\n","        self.dropout = Dropout(dropout)\n","        self.relu = ReLU()\n","        self.n_output = n_output\n","\n","        self.conv1 = GATConv(num_features_xd, dim, heads=heads, concat=True)\n","        self.bn1 = BatchNorm1d(dim * heads)\n","        self.conv2 = GATConv(dim * heads, dim, heads=heads, concat=True)\n","        self.conv3 = GATConv(dim * heads, dim, heads=heads, concat=True)\n","        self.fc1_xd = Linear(dim * heads, output_dim)\n","\n","        # Additional Features Fully Connected Layer\n","        self.fc_additional = Linear(num_additional_features, output_dim)\n","\n","        # Concatenation\n","        self.fc1 = Linear(2 * output_dim, 1024)\n","        self.fc2 = Linear(1024, 256)\n","        self.out = Linear(256, self.n_output)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","        additional_features = data.additional_features.view(-1, 66)\n","\n","        # Molecular features\n","        x = F.elu(self.conv1(x, edge_index))\n","        x = self.bn1(x)\n","        x = F.elu(self.conv2(x, edge_index))\n","        x = F.elu(self.conv3(x, edge_index))\n","        x = global_add_pool(x, batch)\n","        x = F.elu(self.fc1_xd(x))\n","        x = self.dropout(x)\n","\n","        # Process additional features\n","        additional_out = self.fc_additional(additional_features)\n","        additional_out = F.elu(additional_out)\n","        additional_out = self.dropout(additional_out)\n","\n","        # Concatenation\n","        xc = torch.cat((x, additional_out), 1)\n","        xc = self.fc1(xc)\n","        xc = self.relu(xc)\n","        xc = self.dropout(xc)\n","        xc = self.fc2(xc)\n","        xc = self.relu(xc)\n","        xc = self.dropout(xc)\n","        out = self.out(xc)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def custom_loss(y_pred, y_true):\n","    loss1 = nn.mse_loss(y_pred[:, 0].flatten(), y_true[:, 0].flatten())\n","    loss2 = nn.mse_loss(y_pred[:, 1].flatten(), y_true[:, 1].flatten())\n","    return (loss1**0.5 + loss2**0.5) / 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def training(model, device, train_loader, optimizer, epoch):\n","    print('Training on {} samples...'.format(len(train_loader.dataset)))\n","    model.train()\n","    for batch_idx, data in enumerate(train_loader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","\n","        # Adjusting the shape of target data to match the output\n","        loss1 = F.mse_loss(output[:, 0].flatten(), data.y[:, 0].to(device).flatten())\n","        loss2 = F.mse_loss(output[:, 1].flatten(), data.y[:, 1].to(device).flatten())\n","        loss = (loss1**0.5 + loss2**0.5) / 2\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % LOG_INTERVAL == 0:\n","            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n","                                                                           batch_idx * train_loader.batch_size,\n","                                                                           len(train_loader.dataset),\n","                                                                           100. * batch_idx / len(train_loader),\n","                                                                           loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predicting(model, device, loader, mode='validation'):\n","    model.eval()\n","\n","    # 학습을 통해 예측된 값을 저장합니다.\n","    total_preds = []\n","\n","    # 실제 값을 저장합니다.\n","    total_labels = []\n","\n","    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            output = model(data)\n","            total_preds.append(output.cpu().numpy())\n","            \n","            # 만약 mode가 'test'가 아니라면 실제 값을 저장\n","            if mode != 'test':\n","                total_labels.append(data.y.cpu().numpy())\n","    \n","    # 리스트를 numpy 배열로 변환합니다.\n","    if mode == 'test':\n","        return np.vstack(total_preds)\n","    else:\n","        return np.vstack(total_labels), np.vstack(total_preds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TRAIN_BATCH_SIZE = CFG['TRAIN_BATCH_SIZE']\n","TEST_BATCH_SIZE = CFG['TEST_BATCH_SIZE']\n","LR = CFG['LR']\n","LOG_INTERVAL = CFG['LOG_INTERVAL']\n","NUM_EPOCHS = CFG['NUM_EPOCHS']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def rmse(y,f):\n","    rmse = np.sqrt(((y - f)**2).mean(axis=0))\n","    return rmse"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=TRAIN_BATCH_SIZE)\n","\n","model = GATNet().to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n","\n","model_file_name = 'GATNet.model'\n","\n","for epoch in range(NUM_EPOCHS):\n","    training(model, device, train_loader, optimizer, epoch+1)\n","    G, P = predicting(model, device, val_loader)\n","    ret = rmse(G,P)\n","\n","    print('rmse at epoch {}; best_rmse: {:.4f}'.format(epoch, ret))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds = predicting(model, device, test_loader, mode='test')\n","preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_submission = pd.read_csv('Data/sample_submission.csv')\n","sample_submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_submission['MLM'] = preds[:, 0]\n","sample_submission['HLM'] = preds[:, 1]\n","sample_submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
